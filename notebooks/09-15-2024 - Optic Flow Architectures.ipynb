{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optic Flow Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpticFlowDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, input_size):\n",
    "        super(OpticFlowDecoder, self).__init__()\n",
    "\n",
    "        # First layer: input D=34 channels, output 8 channels\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=5, padding=2)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(8)\n",
    "\n",
    "        # Second layer: input 8 channels, output 3 channels (optic flow + normalization)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=3, kernel_size=5, padding=2)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(3)\n",
    "\n",
    "        # Activation function: Softplus\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # Initialize weights homogeneously at 0.001\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                nn.init.constant_(layer.weight, 0.001)\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.constant_(layer.bias, 0.001)\n",
    "            elif isinstance(layer, nn.BatchNorm2d):\n",
    "                nn.init.constant_(layer.weight, 1)\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First conv layer\n",
    "        x = self.batchnorm1(self.conv1(x))\n",
    "        x = self.softplus(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Second conv layer\n",
    "        x = self.batchnorm2(self.conv2(x))\n",
    "        x = self.softplus(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Split the output into the 2D flow (2 channels) and normalization (1 channel)\n",
    "        flow = x[:, :2, :, :]  # First 2 channels are the flow (x and y)\n",
    "        normalization = x[:, 2:3, :, :]  # Third channel is the normalization\n",
    "\n",
    "        flow = flow / (normalization + 1e-8)\n",
    "\n",
    "        # Resize flow to match original input size\n",
    "        flow = F.interpolate(flow, size=input_size, mode='bilinear')\n",
    "\n",
    "        return flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After concatenation: torch.Size([1, 6, 16, 16])\n",
      "After conv1: torch.Size([1, 64, 16, 16])\n",
      "After first residual block: torch.Size([1, 64, 16, 16])\n",
      "After second residual block: torch.Size([1, 128, 8, 8])\n",
      "Final output shape: torch.Size([1, 2, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "class Net_2_layers(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net_2_layers, self).__init__()\n",
    "        \n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=6, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # First residual block\n",
    "        self.conv2_1 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2_1 = nn.BatchNorm2d(64)\n",
    "        self.conv2_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2_2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Second residual block\n",
    "        self.conv3_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn3_1 = nn.BatchNorm2d(128)\n",
    "        self.conv3_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn3_2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Adjust input channels to match the output channels (1x1 conv)\n",
    "        self.adjust_channels = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=1, stride=2, padding=0, bias=False)\n",
    "        \n",
    "        # OpticFlowDecoder\n",
    "        self.optic_flow_decoder = OpticFlowDecoder(in_channels=128, input_size=input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if input is list, combine batch dimension\n",
    "        is_list = isinstance(x, tuple) or isinstance(x, list)\n",
    "        if is_list:\n",
    "            batch_dim = x[0].shape[1]\n",
    "            x = torch.cat(x, dim=1)\n",
    "        print(f\"After concatenation: {x.shape}\")\n",
    "\n",
    "        # First conv layer\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        print(f\"After conv1: {x.shape}\")\n",
    "\n",
    "        # First residual block\n",
    "        residual = x  # Store residual\n",
    "        x = self.relu(self.bn2_1(self.conv2_1(x)))\n",
    "        x = self.bn2_2(self.conv2_2(x))\n",
    "        x += residual  # Add skip connection\n",
    "        x = self.relu(x)\n",
    "        print(f\"After first residual block: {x.shape}\")\n",
    "\n",
    "        # Second residual block\n",
    "        residual = x  # Store residual again\n",
    "        x = self.relu(self.bn3_1(self.conv3_1(x)))\n",
    "        x = self.bn3_2(self.conv3_2(x))\n",
    "        \n",
    "        # Adjust the dimensions of the residual if necessary\n",
    "        residual = self.adjust_channels(residual)\n",
    "        x += residual  # Add skip connection\n",
    "        x = self.relu(x)\n",
    "        print(f\"After second residual block: {x.shape}\")\n",
    "\n",
    "        # Pass through OpticFlowDecoder\n",
    "        flow = self.optic_flow_decoder(x)\n",
    "\n",
    "        return flow\n",
    "\n",
    "# Example usage\n",
    "input_size = 16\n",
    "model = Net_2_layers(input_size)\n",
    "input_tensors = (torch.randn(1, 3, input_size, input_size), torch.randn(1, 3, input_size, input_size))\n",
    "output = model(input_tensors)\n",
    "print(f\"Final output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16 x 16 pixels, 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After conv1: torch.Size([1, 64, 16, 16])\n",
      "After layer1: torch.Size([1, 64, 16, 16])\n",
      "After layer2: torch.Size([1, 128, 8, 8])\n",
      "After layer3: torch.Size([1, 256, 4, 4])\n",
      "Final output shape: torch.Size([1, 2, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "class Net_3_16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_3_16, self).__init__()\n",
    "        resnet = models.resnet18(weights=None)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=1, padding=3, bias=False)  # Modified stride=1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "\n",
    "        self.layer1 = resnet.layer1  # First residual block\n",
    "        self.layer2 = resnet.layer2  # Second residual block\n",
    "        self.layer3 = resnet.layer3  # Third residual block\n",
    "\n",
    "        # Reduce channels to 34 before passing to OpticFlowDecoder\n",
    "        self.conv_reduce = nn.Conv2d(in_channels=256, out_channels=34, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # OpticFlowDecoder\n",
    "        self.optic_flow_decoder = OpticFlowDecoder()\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.conv1(x)\n",
    "        print(f\"After conv1: {x.shape}\")\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        print(f\"After layer1: {x.shape}\")\n",
    "        x = self.layer2(x)\n",
    "        print(f\"After layer2: {x.shape}\")\n",
    "        x = self.layer3(x)\n",
    "        print(f\"After layer3: {x.shape}\")\n",
    "        \n",
    "         # Reduce channels to match OpticFlowDecoder input\n",
    "        x = self.conv_reduce(x)\n",
    "\n",
    "        # Pass through OpticFlowDecoder\n",
    "        flow = self.optic_flow_decoder(x)\n",
    "\n",
    "        # Resize flow to match original input size\n",
    "        flow = F.interpolate(flow, size=(16, 16), mode='bilinear')\n",
    "\n",
    "        \n",
    "\n",
    "        return flow\n",
    "\n",
    "# Example usage\n",
    "model = Net_3_16()\n",
    "input_tensor = torch.randn(1, 3, 16, 16) \n",
    "output = model(input_tensor)\n",
    "print(f\"Final output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 32 x 32 pixels, 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After conv1: torch.Size([1, 64, 32, 32])\n",
      "After layer1: torch.Size([1, 64, 32, 32])\n",
      "After layer2: torch.Size([1, 128, 16, 16])\n",
      "Final output shape: torch.Size([1, 2, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "class Net_2_32(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_2_32, self).__init__()\n",
    "        resnet = models.resnet18(weights=None)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=1, padding=3, bias=False)  # Modified stride=1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "\n",
    "        self.layer1 = resnet.layer1  # First residual block\n",
    "        self.layer2 = resnet.layer2  # Second residual block\n",
    "\n",
    "        # Reduce channels to 34 before passing to OpticFlowDecoder\n",
    "        self.conv_reduce = nn.Conv2d(in_channels=128, out_channels=34, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # OpticFlowDecoder\n",
    "        self.optic_flow_decoder = OpticFlowDecoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        print(f\"After conv1: {x.shape}\")\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        print(f\"After layer1: {x.shape}\")\n",
    "        x = self.layer2(x)\n",
    "        print(f\"After layer2: {x.shape}\")\n",
    "\n",
    "         # Reduce channels to match OpticFlowDecoder input\n",
    "        x = self.conv_reduce(x)\n",
    "\n",
    "        # Pass through OpticFlowDecoder\n",
    "        flow = self.optic_flow_decoder(x)\n",
    "\n",
    "        # Resize flow to match original input size\n",
    "        flow = F.interpolate(flow, size=(32, 32), mode='bilinear')\n",
    "\n",
    "        return flow\n",
    "\n",
    "# Example usage\n",
    "model = Net_2_32()\n",
    "input_tensor = torch.randn(1, 3, 32, 32)\n",
    "output = model(input_tensor)\n",
    "print(f\"Final output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 32 x 32 pixels, 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After conv1: torch.Size([1, 64, 32, 32])\n",
      "After layer1: torch.Size([1, 64, 32, 32])\n",
      "After layer2: torch.Size([1, 128, 16, 16])\n",
      "After layer3: torch.Size([1, 256, 8, 8])\n",
      "Final output shape: torch.Size([1, 2, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "class Net_3_32(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_3_32, self).__init__()\n",
    "        resnet = models.resnet18(weights=None)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=1, padding=3, bias=False)  # Modified stride=1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "\n",
    "        self.layer1 = resnet.layer1  # First residual block\n",
    "        self.layer2 = resnet.layer2  # Second residual block\n",
    "        self.layer3 = resnet.layer3  # Third residual block\n",
    "\n",
    "        # Reduce channels to 34 before passing to OpticFlowDecoder\n",
    "        self.conv_reduce = nn.Conv2d(in_channels=256, out_channels=34, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # OpticFlowDecoder\n",
    "        self.optic_flow_decoder = OpticFlowDecoder()\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.conv1(x)\n",
    "        print(f\"After conv1: {x.shape}\")\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        print(f\"After layer1: {x.shape}\")\n",
    "        x = self.layer2(x)\n",
    "        print(f\"After layer2: {x.shape}\")\n",
    "        x = self.layer3(x)\n",
    "        print(f\"After layer3: {x.shape}\")\n",
    "        \n",
    "         # Reduce channels to match OpticFlowDecoder input\n",
    "        x = self.conv_reduce(x)\n",
    "\n",
    "        # Pass through OpticFlowDecoder\n",
    "        flow = self.optic_flow_decoder(x)\n",
    "\n",
    "        # Resize flow to match original input size\n",
    "        flow = F.interpolate(flow, size=(32, 32), mode='bilinear')\n",
    "\n",
    "        return flow\n",
    "\n",
    "# Example usage\n",
    "model = Net_3_32()\n",
    "input_tensor = torch.randn(1, 3, 32, 32)  \n",
    "output = model(input_tensor)\n",
    "print(f\"Final output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 64 x 64 pixels, 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After conv1: torch.Size([1, 64, 64, 64])\n",
      "After layer1: torch.Size([1, 64, 64, 64])\n",
      "After layer2: torch.Size([1, 128, 32, 32])\n",
      "Final output shape: torch.Size([1, 2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "class Net_2_64(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_2_64, self).__init__()\n",
    "        resnet = models.resnet18(weights=None)\n",
    "\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=1, padding=3, bias=False)  # Modified stride=1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "\n",
    "       \n",
    "        self.layer1 = resnet.layer1  # First residual block\n",
    "        self.layer2 = resnet.layer2  # Second residual block\n",
    "\n",
    "        # Reduce channels to 34 before passing to OpticFlowDecoder\n",
    "        self.conv_reduce = nn.Conv2d(in_channels=128, out_channels=34, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # OpticFlowDecoder\n",
    "        self.optic_flow_decoder = OpticFlowDecoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        x = self.conv1(x)\n",
    "        print(f\"After conv1: {x.shape}\")\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        print(f\"After layer1: {x.shape}\")\n",
    "        x = self.layer2(x)\n",
    "        print(f\"After layer2: {x.shape}\")\n",
    "\n",
    "         # Reduce channels to match OpticFlowDecoder input\n",
    "        x = self.conv_reduce(x)\n",
    "\n",
    "        # Pass through OpticFlowDecoder\n",
    "        flow = self.optic_flow_decoder(x)\n",
    "\n",
    "        # Resize flow to match original input size\n",
    "        flow = F.interpolate(flow, size=(64, 64), mode='bilinear')\n",
    "\n",
    "        return flow\n",
    "\n",
    "# Example usage\n",
    "model = Net_2_64()\n",
    "input_tensor = torch.randn(1, 3, 64, 64)\n",
    "output = model(input_tensor)\n",
    "print(f\"Final output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 64 x 64 pixels, 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After conv1: torch.Size([1, 64, 64, 64])\n",
      "After layer1: torch.Size([1, 64, 64, 64])\n",
      "After layer2: torch.Size([1, 128, 32, 32])\n",
      "After layer3: torch.Size([1, 256, 16, 16])\n",
      "Final output shape: torch.Size([1, 2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "class Net_3_64(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_3_64, self).__init__()\n",
    "  \n",
    "        resnet = models.resnet18(weights=None)\n",
    "\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=1, padding=3, bias=False)  # Modified stride=1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "\n",
    "\n",
    "        self.layer1 = resnet.layer1  # First residual block\n",
    "        self.layer2 = resnet.layer2  # Second residual block\n",
    "        self.layer3 = resnet.layer3  # Third residual block\n",
    "\n",
    "        # Reduce channels to 34 before passing to OpticFlowDecoder\n",
    "        self.conv_reduce = nn.Conv2d(in_channels=256, out_channels=34, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # OpticFlowDecoder\n",
    "        self.optic_flow_decoder = OpticFlowDecoder()\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.conv1(x)\n",
    "        print(f\"After conv1: {x.shape}\")\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        print(f\"After layer1: {x.shape}\")\n",
    "        x = self.layer2(x)\n",
    "        print(f\"After layer2: {x.shape}\")\n",
    "        x = self.layer3(x)\n",
    "        print(f\"After layer3: {x.shape}\")\n",
    "\n",
    "         # Reduce channels to match OpticFlowDecoder input\n",
    "        x = self.conv_reduce(x)\n",
    "\n",
    "        # Pass through OpticFlowDecoder\n",
    "        flow = self.optic_flow_decoder(x)\n",
    "\n",
    "        # Resize flow to match original input size\n",
    "        flow = F.interpolate(flow, size=(64, 64), mode='bilinear')\n",
    "\n",
    "        return flow\n",
    "\n",
    "# Example usage\n",
    "model = Net_3_64()\n",
    "input_tensor = torch.randn(1, 3, 64, 64) \n",
    "output = model(input_tensor)\n",
    "print(f\"Final output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions:\n",
    "- Is this the right amount of layers? (I added first input layer, and then first 2-3 layers from ResNet18) Or should the first layer just be layer1 from ResNet?\n",
    "- The ResNet layers have downsampling in them, did you mean no downsampling in the input layer or in general?\n",
    "- ResNet input layer uses stride=2, padding=3, kernel=7. Since this is for smaller resolution of images, should we change kernel=3 (padding=1), (I already changed stride=1).\n",
    "- For the Optic Flow Decoder, I used what was described in the fly paper. It takes 34 features, Why?\n",
    "- Is the fly decoder what I was supposed to use, or is there another one I should try?\n",
    "- NN is taking one frame at a time, when are the two frames passed to the optic flow decoder?\n",
    "- I am reducing the channels after the ResNet layers to fit D=34, is that ok, or should I change the OFDecoder to take in what the CNN outputs (64 or 128)?\n",
    "- I am not outputting .flo files, how do I compare to my ouptut/ should I be also outputting .flo files? https://stackoverflow.com/questions/28013200/reading-middlebury-flow-files-with-python-bytes-array-numpy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
