{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "LOCAL_IMAGE_DIRECTORY = \"/Users/Shared/Files From c.localized/Work/Code/Virtual Zebrafish/YouTube Movies/Frames/\"\n",
    "FILENAME = \"frame_0052.jpg\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Function to display tensor as image\n",
    "def show_image(tensor_img):\n",
    "    # Remove the batch dimension\n",
    "    img = tensor_img.squeeze(0)\n",
    "    \n",
    "    # Transpose the tensor dimensions to (height, width, channels)\n",
    "    img = img.permute(1, 2, 0)\n",
    "    \n",
    "    # Convert the tensor to a NumPy array and detach it from the computation graph\n",
    "    img = img.detach().numpy()\n",
    "    \n",
    "    # Display the image using matplotlib\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')  # Turn off axis numbers and ticks\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to load an image and convert it to a PyTorch tensor\n",
    "def load_image(file_path):\n",
    "\n",
    "    # Open the image file\n",
    "    with Image.open(file_path).convert('RGB') as img:  # Convert to RGB for 3 channels\n",
    "        # Define transformations: resize to 224x224 and then convert to tensor\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((25, 25)),  # Resize to 224x224 pixels\n",
    "            transforms.ToTensor()  # Convert the PIL Image to a tensor\n",
    "        ])\n",
    "\n",
    "        \n",
    "        \n",
    "        # Apply the transformations to the image and add a batch dimension\n",
    "        img_tensor = transform(img).unsqueeze(0)\n",
    "        \n",
    "        return img_tensor\n",
    "\n",
    "# Example usage\n",
    "file_path = LOCAL_IMAGE_DIRECTORY + FILENAME\n",
    "\n",
    "tensor_img = load_image(file_path)\n",
    "\n",
    "# Verify the shape (should be [1, 3, 224, 224] for a single RGB image)\n",
    "#print(tensor_img.shape)\n",
    "\n",
    "# Now you can pass `tensor_img` into your neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "class ConvReluNet(nn.Module):\n",
    "    def __init__(self, output_size, training=True):\n",
    "        super(ConvReluNet, self).__init__()\n",
    "\n",
    "        # First Convolutional Layer followed by ReLU activation\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Second Convolutional Layer followed by ReLU activation\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Fully Connected Layer (for training)\n",
    "        # after two conv layers with same padding, the size remains 25x25\n",
    "        self.fc = nn.Linear(in_features=64*25*25, out_features=output_size)\n",
    "\n",
    "        # State for adding the fully connected layer or not\n",
    "        self.training_mode = training\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "\n",
    "        # If the network is in training mode, then add the dense layer\n",
    "        if self.training_mode:\n",
    "            x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "            x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Usage\n",
    "output_size = 10  # Specify your desired output size\n",
    "net = ConvReluNet(output_size)\n",
    "\n",
    "# Example input\n",
    "\n",
    "output = net(tensor_img)\n",
    "\n",
    "print(output.shape)  # Should be torch.Size([1, output_size])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number_from_filename(fname):\n",
    "    return int(fname.split('_')[1].split('.')[0])\n",
    "\n",
    "\n",
    "class TripletDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder_path):\n",
    "        self.image_paths = sorted([os.path.join(folder_path, fname) for fname in os.listdir(folder_path) if fname.endswith('.jpg')], key=lambda x: extract_number_from_filename(os.path.basename(x)))\n",
    "        self.pos_margin = 20\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((25, 25)),  # Resize to 25 by 25 pixels\n",
    "            transforms.ToTensor()  # Convert the PIL Image to a tensor\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load image utility function\n",
    "        def load_image(img_path):\n",
    "            with Image.open(img_path) as img:\n",
    "                return self.transform(img)\n",
    "        \n",
    "        # Anchor Image\n",
    "        anchor_img = load_image(self.image_paths[index])\n",
    "        \n",
    "        # Positive Image within [margin] frames of the anchor\n",
    "        positive_index = random.choice(list(range(max(0, index-self.pos_margin), index)) + list(range(index+1, min(len(self.image_paths), index+self.pos_margin+1))))\n",
    "        positive_img = load_image(self.image_paths[positive_index])\n",
    "        \n",
    "        # Negative Image more than [margin] frames away from anchor\n",
    "        negative_indices = list(range(0, max(0, index-self.pos_margin))) + list(range(min(len(self.image_paths), index+self.pos_margin +1), len(self.image_paths)))\n",
    "        negative_index = random.choice(negative_indices)\n",
    "        negative_img = load_image(self.image_paths[negative_index])\n",
    "\n",
    "        #print(f\"Anchor index is {index}, Positive index is {positive_index}, negative index is {negative_index}\")\n",
    "\n",
    "        return anchor_img, positive_img, negative_img\n",
    "\n",
    "folder_path = LOCAL_IMAGE_DIRECTORY\n",
    "triplet_dataset = TripletDataset(folder_path)\n",
    "triplet_loader = torch.utils.data.DataLoader(dataset=triplet_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Example usage:\n",
    "anchor_imgs, positive_imgs, negative_imgs = next(iter(triplet_loader))\n",
    "z_a = net(anchor_imgs)\n",
    "z_p = net(positive_imgs)\n",
    "z_n = net(negative_imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoNCELoss(nn.Module):\n",
    "    def __init__(self, tau=0.1):\n",
    "        \"\"\"\n",
    "        Initialize InfoNCE Loss module.\n",
    "\n",
    "        Parameters:\n",
    "        - tau : Temperature parameter for scaling logits\n",
    "        \"\"\"\n",
    "        super(InfoNCELoss, self).__init__()\n",
    "        self.tau = tau\n",
    "\n",
    "    def forward(self, z_a, z_p, z_n):\n",
    "        \"\"\"\n",
    "        Compute the InfoNCE loss.\n",
    "\n",
    "        Parameters:\n",
    "        - z_a : Tensor of shape (batch_size, feature_dim) representing anchor points\n",
    "        - z_p : Tensor of shape (batch_size, feature_dim) representing positive examples\n",
    "        - z_n : Tensor of shape (batch_size, feature_dim) representing negative examples\n",
    "\n",
    "        Returns:\n",
    "        - Loss : InfoNCE loss value\n",
    "        \"\"\"\n",
    "        # Compute the similarity (dot product) for positive and negative pairs\n",
    "        sim_pos = torch.sum(z_a * z_p, dim=-1)  # (batch_size,)\n",
    "        sim_neg = torch.sum(z_a * z_n, dim=-1)  # (batch_size,)\n",
    "\n",
    "        # Calculate the logits: concatenate the similarities and divide by the temperature\n",
    "        logits = torch.cat([sim_pos.unsqueeze(1), sim_neg.unsqueeze(1)], dim=1) / self.tau\n",
    "\n",
    "        # Define the labels (anchor and positive are always 0 since they are the \"true\" match)\n",
    "        labels = torch.zeros(logits.size(0), dtype=torch.long).to(z_a.device)\n",
    "\n",
    "        # Compute the cross entropy loss\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6849, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infonce = InfoNCELoss()\n",
    "infonce(z_a,z_p,z_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 1.4844\n",
      "Epoch [2/3], Loss: 0.2670\n",
      "Epoch [3/3], Loss: 0.1231\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "output_size = 10  # Specify your desired output size\n",
    "model = ConvReluNet(output_size)\n",
    "# Assuming your model is called 'model' and it's on the desired device (e.g., 'cuda')\n",
    "model = model.to('mps')\n",
    "model.train()\n",
    "\n",
    "folder_path = LOCAL_IMAGE_DIRECTORY\n",
    "triplet_dataset = TripletDataset(folder_path)\n",
    "dataloader = torch.utils.data.DataLoader(dataset=triplet_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "\n",
    "loss_function = InfoNCELoss()\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for anchors, positives, negatives in dataloader:\n",
    "        # Move the data to the desired device\n",
    "        anchors = anchors.to('mps')\n",
    "        positives = positives.to('mps')\n",
    "        negatives = negatives.to('mps')\n",
    "\n",
    "        # Forward pass\n",
    "        anchor_out = model(anchors)\n",
    "        positive_out = model(positives)\n",
    "        negative_out = model(negatives)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_function(anchor_out, positive_out, negative_out)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
