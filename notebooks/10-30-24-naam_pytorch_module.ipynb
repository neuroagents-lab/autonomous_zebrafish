{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Modified ReducedNANLayer\n",
        "class ReducedNANLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Reduced neuron-astrocyte network without symmetry assumptions.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, dt, steps, device, clamp=False, store_intermediate=False):\n",
        "        super(ReducedNANLayer, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.dt = dt\n",
        "        self.steps = steps\n",
        "        self.device = device\n",
        "        self.clamp = clamp\n",
        "        self.store_intermediate = store_intermediate\n",
        "\n",
        "        # Activation functions\n",
        "        self.phi = F.relu  # neuron activation\n",
        "        self.g = F.relu    # synapse activation\n",
        "        self.psi = F.relu  # process activation\n",
        "\n",
        "        # Time constants and factors for Euler integration\n",
        "        self.tau_x, self.tau_s, self.tau_p = 1.0, 1.0, 1.0\n",
        "        self.alpha_x = self.dt / self.tau_x\n",
        "        self.alpha_s = self.dt / self.tau_s\n",
        "        self.alpha_p = self.dt / self.tau_p\n",
        "\n",
        "        # Network weights and biases\n",
        "        self.W_xx = nn.Linear(input_size, input_size, bias=False, device=device)  # neuron-to-neuron\n",
        "        self.W_xs = nn.Linear(input_size, input_size, bias=False, device=device)  # neuron-to-synapse\n",
        "        self.W_sp = nn.Parameter(torch.empty(input_size, device=device)) # synapse-to-process\n",
        "        self.W_ps = nn.Parameter(torch.empty(input_size, device=device)) # process-to-synapse\n",
        "        nn.init.kaiming_normal_(self.W_sp.unsqueeze(1))\n",
        "        nn.init.kaiming_normal_(self.W_ps.unsqueeze(1))\n",
        "        self.W_pp = nn.Linear(input_size, input_size, bias=False, device=device)  # process-to-process\n",
        "\n",
        "    def forward(self, inp, x0=None, s0=None, p0=None, free_inds=None):\n",
        "\n",
        "        # Initialize states if not provided\n",
        "        if x0 is None:\n",
        "            x0 = torch.zeros_like(inp, device=self.device)\n",
        "        if s0 is None:\n",
        "            s0 = torch.zeros_like(inp, device=self.device)\n",
        "        if p0 is None:\n",
        "            p0 = torch.zeros_like(inp, device=self.device)\n",
        "\n",
        "        x, s, p = x0.clone(), s0.clone(), p0.clone()\n",
        "        xs = []\n",
        "\n",
        "        for _ in range(self.steps):\n",
        "            # Activations\n",
        "            phi_t = self.phi(x)\n",
        "            g_t = self.g(s)\n",
        "            psi_t = self.psi(p)\n",
        "\n",
        "            # State updates\n",
        "            dx = -x + self.W_xx(g_t * phi_t) + inp\n",
        "            ds = -s + self.W_ps * psi_t + self.W_xs(phi_t)\n",
        "            dp = -p + self.W_pp(psi_t) + self.W_sp * g_t\n",
        "\n",
        "            # Euler integration with optional clamping\n",
        "            x = x + self.alpha_x * dx * (free_inds if free_inds is not None else 1)\n",
        "            s = s + self.alpha_s * ds\n",
        "            p = p + self.alpha_p * dp\n",
        "\n",
        "            if self.store_intermediate:\n",
        "                xs.append(x)\n",
        "\n",
        "        xs = torch.stack(xs) if self.store_intermediate else None\n",
        "        return x, xs\n",
        "\n",
        "# Modified ReducedNAN\n",
        "class ReducedNAN(nn.Module):\n",
        "    \"\"\"\n",
        "    A module that applies a read-in layer, then the ReducedNAN model, and finally a linear readout layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, output_size, dt, steps, device, clamp=False, store_intermediate=False):\n",
        "        super(ReducedNAN, self).__init__()\n",
        "\n",
        "        # Read-in layer to transform input to hidden dimension\n",
        "        self.read_in = nn.Linear(input_size, hidden_size, device=device)\n",
        "\n",
        "        # Instantiate the ReducedNAN model\n",
        "        self.reduced_nan = ReducedNANLayer(hidden_size, dt, steps, device, clamp, store_intermediate)\n",
        "\n",
        "        # Output layer that maps the final state to the desired output size\n",
        "        self.readout = nn.Linear(hidden_size, output_size, device=device)\n",
        "\n",
        "    def forward(self, inp, x0=None, s0=None, p0=None, free_inds=None):\n",
        "        # Apply read-in layer\n",
        "        inp_transformed = self.read_in(inp)\n",
        "\n",
        "        # Pass through ReducedNAN model\n",
        "        final_state, intermediate_states = self.reduced_nan(inp_transformed, x0, s0, p0, free_inds)\n",
        "\n",
        "        # Apply readout layer to the final output\n",
        "        output = self.readout(final_state)\n",
        "\n",
        "        return output, intermediate_states\n",
        "\n",
        "# Training code\n",
        "def train_mnist():\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    # Hyperparameters\n",
        "    input_size = 28 * 28  # MNIST images are 28x28\n",
        "    hidden_size = 128\n",
        "    output_size = 10  # 10 classes for MNIST digits\n",
        "    dt = 0.1\n",
        "    steps = 10\n",
        "    clamp = False\n",
        "    store_intermediate = False\n",
        "    num_epochs = 5\n",
        "    batch_size = 64\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    # MNIST dataset\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "    test_dataset  = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader  = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model, loss function, optimizer\n",
        "    model = ReducedNAN(input_size, hidden_size, output_size, dt, steps, device, clamp, store_intermediate).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    total_step = len(train_loader)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            # Flatten images to [batch_size, input_size]\n",
        "            images = images.view(-1, 28*28).to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs, _ = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (batch_idx+1) % 100 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{total_step}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Testing the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.view(-1, 28*28).to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs, _ = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        print(f'Test Accuracy of the model on the 10000 test images: {100 * correct / total} %')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_mnist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUddZIzQUFqF",
        "outputId": "960687cb-9500-4a83-f4e9-21a62bd10173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.08MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 133kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:06<00:00, 241kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.52MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Epoch [1/5], Step [100/938], Loss: 0.5124\n",
            "Epoch [1/5], Step [200/938], Loss: 0.2797\n",
            "Epoch [1/5], Step [300/938], Loss: 0.2140\n",
            "Epoch [1/5], Step [400/938], Loss: 0.1360\n",
            "Epoch [1/5], Step [500/938], Loss: 0.1659\n",
            "Epoch [1/5], Step [600/938], Loss: 0.1865\n",
            "Epoch [1/5], Step [700/938], Loss: 0.0621\n",
            "Epoch [1/5], Step [800/938], Loss: 0.3131\n",
            "Epoch [1/5], Step [900/938], Loss: 0.2631\n",
            "Epoch [2/5], Step [100/938], Loss: 0.1227\n",
            "Epoch [2/5], Step [200/938], Loss: 0.0797\n",
            "Epoch [2/5], Step [300/938], Loss: 0.0770\n",
            "Epoch [2/5], Step [400/938], Loss: 0.0355\n",
            "Epoch [2/5], Step [500/938], Loss: 0.2166\n",
            "Epoch [2/5], Step [600/938], Loss: 0.0372\n",
            "Epoch [2/5], Step [700/938], Loss: 0.1248\n",
            "Epoch [2/5], Step [800/938], Loss: 0.1235\n",
            "Epoch [2/5], Step [900/938], Loss: 0.2347\n",
            "Epoch [3/5], Step [100/938], Loss: 0.1287\n",
            "Epoch [3/5], Step [200/938], Loss: 0.0401\n",
            "Epoch [3/5], Step [300/938], Loss: 0.2082\n",
            "Epoch [3/5], Step [400/938], Loss: 0.1907\n",
            "Epoch [3/5], Step [500/938], Loss: 0.3698\n",
            "Epoch [3/5], Step [600/938], Loss: 0.1340\n",
            "Epoch [3/5], Step [700/938], Loss: 0.1031\n",
            "Epoch [3/5], Step [800/938], Loss: 0.0637\n",
            "Epoch [3/5], Step [900/938], Loss: 0.0691\n",
            "Epoch [4/5], Step [100/938], Loss: 0.0331\n",
            "Epoch [4/5], Step [200/938], Loss: 0.0764\n",
            "Epoch [4/5], Step [300/938], Loss: 0.0610\n",
            "Epoch [4/5], Step [400/938], Loss: 0.0707\n",
            "Epoch [4/5], Step [500/938], Loss: 0.0855\n",
            "Epoch [4/5], Step [600/938], Loss: 0.0598\n",
            "Epoch [4/5], Step [700/938], Loss: 0.0736\n",
            "Epoch [4/5], Step [800/938], Loss: 0.0175\n",
            "Epoch [4/5], Step [900/938], Loss: 0.0875\n",
            "Epoch [5/5], Step [100/938], Loss: 0.1074\n",
            "Epoch [5/5], Step [200/938], Loss: 0.0252\n",
            "Epoch [5/5], Step [300/938], Loss: 0.0042\n",
            "Epoch [5/5], Step [400/938], Loss: 0.0755\n",
            "Epoch [5/5], Step [500/938], Loss: 0.0632\n",
            "Epoch [5/5], Step [600/938], Loss: 0.0436\n",
            "Epoch [5/5], Step [700/938], Loss: 0.0280\n",
            "Epoch [5/5], Step [800/938], Loss: 0.1288\n",
            "Epoch [5/5], Step [900/938], Loss: 0.0260\n",
            "Test Accuracy of the model on the 10000 test images: 97.3 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NnT6sMiVX1hK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}